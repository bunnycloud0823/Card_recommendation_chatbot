import streamlit as st
import json
import os
import re
import random
import datetime
from dotenv import load_dotenv
from card_rag import search_card
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationBufferMemory
from langchain_core.runnables import RunnableLambda

load_dotenv()

LOG_PATH = "./user_logs.jsonl"
AB_VERSION = random.choice(["A", "B"])
SESSION_START = datetime.datetime.now()

# ------------------------------- ì¹´ë“œ ë§í¬Â·ì´ë¯¸ì§€ ë¡œë“œ -------------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
LINK_IMAGE_PATH = os.path.join(BASE_DIR, "cards_link_image.json")
with open(LINK_IMAGE_PATH, "r", encoding="utf-8") as f:
    link_data = json.load(f)

LINK_DB = {str(item["card_id"]): item for item in link_data}


def extract_card_ids(text):
    """AI ì‘ë‹µì—ì„œ ì¹´ë“œID ì¶”ì¶œ"""
    return re.findall(r"ì¹´ë“œID\s*:\s*(\d+)", text)


def show_card_details(card_ids):
    """ì¹´ë“œID ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€Â·ë§í¬ í‘œì‹œ"""
    for cid in card_ids:
        data = LINK_DB.get(str(cid))
        if not data:
            continue

        img_path = data.get("image")
        if img_path:
            abs_img_path = os.path.normpath(
                os.path.join(BASE_DIR, "..", img_path.replace("./", ""))
            )
            if os.path.exists(abs_img_path):
                st.image(abs_img_path, width=250)
            else:
                st.warning(f"ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {abs_img_path}")

        # ë§í¬ ì¶œë ¥ (ë²„íŠ¼ ëŒ€ì‹  ì§ì ‘ ë§í¬ í‘œì‹œ)
        pc_link = data.get("request_pc")
        m_link = data.get("request_m")

        if pc_link:
            st.markdown(f"[ğŸ–¥ï¸ PC ì‹ ì²­ ë§í¬ ì—´ê¸°]({pc_link})", unsafe_allow_html=True)
        else:
            st.write("PC ì‹ ì²­ ë§í¬ ì—†ìŒ")

        if m_link:
            st.markdown(f"[ğŸ“± ëª¨ë°”ì¼ ì‹ ì²­ ë§í¬ ì—´ê¸°]({m_link})", unsafe_allow_html=True)
        else:
            st.write("ëª¨ë°”ì¼ ì‹ ì²­ ë§í¬ ì—†ìŒ")

        st.write("---")

    return []


# ------------------------------- ì„¸ì…˜ ì´ˆê¸°í™” -------------------------------
if "pre_memory" not in st.session_state:
    st.session_state["pre_memory"] = ConversationBufferMemory(
        memory_key="chat_history", return_messages=True
    )

if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {
            "role": "assistant",
            "content": "ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” AI ì¹´ë“œ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì—ê²Œ ë§ëŠ” ì¹´ë“œë¥¼ ì¶”ì²œí•´ë“œë¦´ê²Œìš”.",
        }
    ]

# ------------------------------- ëª¨ë¸ ì„¤ì • -------------------------------
model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

system_prompt = """
ë„ˆëŠ” ì¹´ë“œì‚¬ ì§ì›ì´ì•¼. ê³ ê°ì˜ ì§ˆì˜ê°€ ë“¤ì–´ì˜¤ë©´ contextì— ë”°ë¼ ê°€ì¥ í˜œíƒì´ 2ê°œ ì¶”ì²œí•´ì¤˜. 
ì‹ ìš©ì¹´ë“œ, ì²´í¬ì¹´ë“œì— ëŒ€í•œ ëª…ì‹œê°€ ì—†ì„ ê²½ìš° ì‹ ìš©ì¹´ë“œ, ì²´í¬ì¹´ë“œ ê°ê° 1ê°œì”© ì¶”ì²œí•˜ê³  ëª…ì‹œí•  ê²½ìš° í•´ë‹¹ ì¹´ë“œë¡œ 2ê°œ ì¶”ì²œí•´ì¤˜.
context ë‚´ìš©ì— í•œí•´ì„œë§Œ ì¶”ì²œí•´ì£¼ë˜, contextì— ì—†ëŠ” ë‚´ìš©ì€ ë°œì„¤í•˜ì§€ ë§ì•„ì¤˜.
ê° ì¹´ë“œì˜ ë§ˆì§€ë§‰ ì¤„ì—ëŠ” ë°˜ë“œì‹œ 'ì¹´ë“œID: {{card_id}}'ë¥¼ í¬í•¨ì‹œì¼œì¤˜.
"""

user_prompt = """\
ì•„ë˜ì˜ ì‚¬ìš©ì questionì„ ì½ê³  contextë¥¼ ì°¸ê³ í•˜ì—¬ ê°€ì¥ ì í•©í•œ ì¹´ë“œ(ì‚¬ìš©ìê°€ í˜œíƒì„ ìµœëŒ€ë¡œ ë°›ì„ ìˆ˜ ìˆëŠ” ì¹´ë“œ)ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.

--chat_history-- 
{chat_history}

--question--
{question}

--context--
{context}
"""

final_prompt = ChatPromptTemplate([("system", system_prompt), ("user", user_prompt)])


def get_user_input(question):
    return {
        "chat_history": st.session_state["pre_memory"].chat_memory.messages,
        "question": question,
        "context": search_card(question),
    }


chain = RunnableLambda(get_user_input) | final_prompt | model | StrOutputParser()


# ------------------------------- ëŒ€í™” í•¨ìˆ˜ -------------------------------
def conversation_with_memory(question, user_info):
    stream_placeholder = st.empty()
    image_placeholder = st.empty()
    full_response = ""

    for chunk in chain.stream(question):
        full_response += chunk
        stream_placeholder.write(full_response)

    # ì¹´ë“œID ì¶”ì¶œ ë° ì´ë¯¸ì§€/ë§í¬ í‘œì‹œ
    card_ids = extract_card_ids(full_response)
    with image_placeholder.container():
        clicked = show_card_details(card_ids)

    session_duration = (datetime.datetime.now() - SESSION_START).total_seconds()

    st.session_state["pre_memory"].save_context(
        {"input": question}, {"output": full_response}
    )

    log_entry = {
        "timestamp": datetime.datetime.now().isoformat(),
        "user_info": user_info,
        "query": question,
        "response": full_response,
        "card_ids": card_ids,
        "clicked_cards": clicked,
        "session_duration_sec": session_duration,
        "ab_version": AB_VERSION,
    }

    os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)
    with open(LOG_PATH, "a", encoding="utf-8") as f:
        f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

    return full_response


# ------------------------------- ë©”ì¸ í™”ë©´ -------------------------------
st.title("ë‹¹ì‹ ë§Œì˜ AI ì¹´ë“œ ì¶”ì²œ ì±—ë´‡ ì„œë¹„ìŠ¤ğŸ¥°")

col1, col2 = st.columns(2)
with col1:
    age_group = st.radio(
        "ì—°ë ¹ëŒ€", ["10ëŒ€", "20ëŒ€", "30ëŒ€", "40ëŒ€", "50ëŒ€ ì´ìƒ"], index=0
    )
with col2:
    occupation = st.radio("ì§ì—…", ["í•™ìƒ", "ì§ì¥ì¸", "ì·¨ì—… ì¤€ë¹„ìƒ", "ê¸°íƒ€"], index=0)
user_name = st.text_input("ì´ë¦„ ë˜ëŠ” ë‹‰ë„¤ì„ì„ ì…ë ¥í•˜ì„¸ìš”:", "")

user_info = {
    "name": user_name or "ìµëª…",
    "age_group": age_group,
    "occupation": occupation,
}

for msg in st.session_state["messages"]:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

question = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
if question:
    st.session_state["messages"].append({"role": "user", "content": question})
    with st.chat_message("user"):
        st.write(question)

if st.session_state["messages"][-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        try:
            ai_response = conversation_with_memory(question, user_info)
            st.session_state["messages"].append(
                {"role": "assistant", "content": ai_response}
            )
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
